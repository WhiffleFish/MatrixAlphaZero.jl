begin
    using MatrixAlphaZero
    const AZ = MatrixAlphaZero
    using JLD2
    using Flux
    using MarkovGames
    using POMDPs
    using POMDPTools
    using POSGModels.Dubin
    using MCTS
    using ExperimentTools
    using Plots
end

game = DubinMG()

b0 = ImplicitDistribution() do rng
    s1 = Dubin.Vec3(rand(rng) * 10, rand(rng) * 10, rand(rng) * 2π)
    s2 = Dubin.Vec3(rand(rng) * 10, rand(rng) * 10, rand(rng) * 2π)
    return JointDubinState(s1, s2)
end

mcts_player = 2
az_player = MarkovGames.other_player(mcts_player)
oracle = AZ.load_oracle(@__DIR__)
planner = AlphaZeroPlanner(game, oracle, max_iter=100)
Flux.loadmodel!(planner, last(readdir(@modeldir; join=true)))
mcts_solver = MCTSSolver(n_iterations=100)
mdp = ExploitabilityMDP(game, AlphaZeroPlanner(game, oracle, max_iter=0), mcts_player)
mcts_pol = solve(mcts_solver, mdp)
s = rand(b0)

behavior_info(planner, s)


az_pol = ExperimentTools.SinglePlayerAlphaZeroPolicy(planner, az_player)
joint_pol = if isone(mcts_player)
    ExperimentTools.JointPolicy(mcts_pol, az_pol)
else
    ExperimentTools.JointPolicy(az_pol, mcts_pol)
end
sim = HistoryRecorder(max_steps=30)

hist = simulate(sim, game, joint_pol)

begin
    h_i = hist[25]
    plot(game, h_i.s, h_i.behavior[1], SparseCat([h_i.behavior[mcts_player].val], [1.0]))
end

begin
    h_i = hist[1]
    plot(game, h_i.s, SparseCat([h_i.behavior[mcts_player].val], [1.0]), h_i.behavior[2])
end

b, info = behavior_info(planner, s)
@profview behavior_info(planner, s)
info.v
